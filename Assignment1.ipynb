{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21d5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import os\n",
    "import random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import operator\n",
    "import nltk \n",
    "from nltk import word_tokenize as tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02ff8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pushpanb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "punc = string.punctuation\n",
    "\n",
    "# Create stopword + punctuation list.\n",
    "stop_puncs = (set([x for x in punc] + list(stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd5e72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 523 files in the training directory: /Users/pushpanb/Downloads/Second Sem/ANLP/lab2resources/sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os,random,math\n",
    "training_dir=\"/Users/pushpanb/Downloads/Second Sem/ANLP/lab2resources/sentence-completion/Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=training_dir,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    #random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "trainingfiles,heldoutfiles=get_training_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c79e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 523 files in the training directory: /Users/pushpanb/Downloads/Second Sem/ANLP/lab2resources/sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "parentdir=\"/Users/pushpanb/Downloads/Second Sem/ANLP/lab2resources/sentence-completion\"\n",
    "trainingdir=os.path.join(parentdir,\"Holmes_Training_Data\")\n",
    "training,testing=get_training_testing(trainingdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0b9be04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "\n",
    "class language_model():\n",
    "    def __init__(self,trainingdir=trainingdir,files=[], test_files=[], construct_params={}):\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        self.test_files = test_files\n",
    "        # Constructor Parameters.\n",
    "        self.construct_params=construct_params\n",
    "        self.verbose = construct_params.get(\"verbose\", False)\n",
    "        self.train()\n",
    "        \n",
    "    def train(self):    \n",
    "        \"\"\"\n",
    "        Method called in model initialization. \n",
    "        Calls \"private\" methods which process files, make unknowns, discount and convert n-gram dictionaries to probabilities. \n",
    "        \"\"\"\n",
    "        self.unigram={}\n",
    "        self.bigram={}\n",
    "        self.trigram={}\n",
    "        self._processfiles()\n",
    "        self._make_unknowns(known=self.construct_params.get(\"known\",2))\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "    def _processline(self,line):\n",
    "        \"\"\"\n",
    "        Method processes lines of txt files and tokenizes sentences contained within.\n",
    "        Information is stored within respective n-gram dictionaries.\n",
    "        \"\"\"\n",
    "        tokens=tokenize(line)\n",
    "        if self.construct_params.get(\"remove_stopwords\",False) == True:\n",
    "            tokens = [token.lower() for token in tokens if token.lower() not in stop_puncs]\n",
    "        if self.construct_params.get(\"lemmatize\", False) == True:\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [\"__START\"] + tokens + [\"__END\"]\n",
    "        previous=\"__END\"\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Unigram\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "            # Bigram\n",
    "            current=self.bigram.get(previous,{})\n",
    "            current[token]=current.get(token,0)+1\n",
    "            self.bigram[previous]=current\n",
    "            previous=token\n",
    "            # Trigram\n",
    "            if i < len(tokens)-2:\n",
    "                # Next words.\n",
    "                next = tokens[i+1] \n",
    "                next_next = tokens[i+2]\n",
    "                # Get dictionaries.\n",
    "                inner = self.trigram.get(token,{})\n",
    "                innermost = inner.get(next,{})\n",
    "                innermost[next_next] = innermost.get(token,0) + 1\n",
    "                # Write frequencies to dictionaries.\n",
    "                inner[next] = innermost\n",
    "                self.trigram[token] = inner\n",
    "    def _processfiles(self):\n",
    "        \"\"\"\n",
    "        Process text files.\n",
    "        \"\"\"\n",
    "        for afile in tqdm.tqdm(self.files):\n",
    "            # print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                if self.verbose:\n",
    "                    print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "                else:\n",
    "                    pass\n",
    "    def _convert_to_probs(self):\n",
    "        \"\"\"\n",
    "        Convert counts to probabilities for each n-gram dictionary.\n",
    "        \"\"\"\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "        self.trigram={k1:{k2:{k3:v/sum(adict2.values()) for k3, v in adict2.items()} for k2, adict2 in adict1.items()} for k1, adict1 in self.trigram.items()}\n",
    "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
    "        #use probabilities according to method to generate a likely next sequence\n",
    "        #choose random token from k best\n",
    "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
    "        most_likely = []\n",
    "        if method==\"unigram\":\n",
    "            dist=self.unigram\n",
    "            #sort the tokens by unigram probability\n",
    "            most_likely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        elif method == \"bigram\":\n",
    "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
    "            most_likely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        elif method == \"trigram\":\n",
    "            # Split context string for first and second context words.\n",
    "            context = current.split()\n",
    "            c1, c2 = context[0], context[1]\n",
    "            dist = self.trigram[c1][c2]\n",
    "            # Get all words with maximum value.\n",
    "            most_likely = [(k, _) for k, v in dist.items() if v == max(dist.values())]\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[w for (w,p) in most_likely if w not in blacklist]\n",
    "        #choose one randomly from the top k\n",
    "        res=random.choice(filtered[:k])\n",
    "        return res\n",
    "    def generate(self,k=20,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n",
    "        \"\"\"\n",
    "        Example sentence generator method: Shannon Visualizations.\n",
    "        k selects from the best top(k)s.\n",
    "        \"\"\"\n",
    "        if method==\"\":\n",
    "            method=methodparams.get(\"method\",\"bigram\")\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        try: \n",
    "          # Trigram\n",
    "          if method==\"trigram\":\n",
    "            # Set current word to first context.\n",
    "            context_1 = current\n",
    "            # Set random choice of next word to second context.\n",
    "            context_2 = random.choice([key for key, adict in self.trigram[current].items()])\n",
    "            # Check end token hasnt been reached.\n",
    "            while context_2 != end and len(tokens)<limit:\n",
    "              # Pass current contexts to next likely method which re splits them in the tri- 4-gram cases.\n",
    "                current = \" \".join([context_1, context_2])\n",
    "                current = self.nextlikely(k=k, current=current, method=method)\n",
    "              # Append word to the list that will eventually be generated.\n",
    "                tokens.append(current)\n",
    "              # Set the the second context to now be first and the predicted word (current) to be next.\n",
    "                context_1 = context_2\n",
    "                context_2 = current\n",
    "            # After loop return the tokens joined by whitespace.\n",
    "            return \" \".join(tokens[:-1])\n",
    "        except:\n",
    "          # If error is thr2mown rerun method until it generates a valid sentence.\n",
    "          return self.generate(k=k,end=end,limit=limit,method=method,methodparams=methodparams)\n",
    "        # Below calls the unigram and bigram versions of the method.\n",
    "        while current!=end and len(tokens)<limit:\n",
    "            current=self.nextlikely(k=k,current=current,method=method)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[:-1])\n",
    "    def get_prob(self,token,context=\"\",methodparams={}):\n",
    "        if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        else:\n",
    "            if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n",
    "                unidist=self.kn\n",
    "            else:\n",
    "                unidist=self.unigram\n",
    "            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "            big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "            lmbda=bigram[\"__DISCOUNT\"]\n",
    "            uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p=big_p+lmbda*uni_p            \n",
    "            return p\n",
    "    def compute_prob_line(self,line,methodparams={}):\n",
    "        \"\"\"\n",
    "        Refactored method which calls get_probs() for uni- and bigram cases. Contains functionality for tri- and 4-gram cases within.\n",
    "        Method is not commented as it should be self explanatory:\n",
    "        Lots of if else statements to fit the contexts into a n-gram dictionary. \n",
    "        \n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "        \"\"\"\n",
    "        tokens=tokenize(line)\n",
    "        if self.construct_params.get(\"remove_stopwords\",False) == True:\n",
    "            tokens = [token.lower() for token in tokens if token.lower() not in stop_puncs]\n",
    "        if self.construct_params.get(\"lemmatize\", False) == True:\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [\"__START\"] + tokens + [\"__END\"]\n",
    "        acc=0\n",
    "        if methodparams.get(\"method\", \"unigram\") in [\"unigram\", \"bigram\"]:\n",
    "            for i,token in enumerate(tokens[1:]):\n",
    "                acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
    "            return acc,len(tokens[1:])\n",
    "        # Trigram. \n",
    "        if methodparams.get(\"method\") == \"trigram\":\n",
    "            try:\n",
    "                for i, token in enumerate(tokens[1:]):\n",
    "                    if i < len(tokens[1:]) - 3 and len(tokens[1:]) >= 3:\n",
    "                        word1, word2, word3 = tokens[i+1], tokens[i+1+1], tokens[i+1+2]\n",
    "                        if word1 in self.trigram:\n",
    "                            if word2 in self.trigram[word1]:\n",
    "                                if word3 in self.trigram[word1][word2]:\n",
    "                                    acc+=math.log(self.trigram[word1][word2][word3])\n",
    "                                else:\n",
    "                                    acc+=math.log(self.trigram[word1][word2][\"__UNK\"])\n",
    "                            else:\n",
    "                                if word3 in self.trigram[word1][\"__UNK\"]:\n",
    "                                    acc+=math.log(self.trigram[word1][\"__UNK\"][word3])\n",
    "                                else: \n",
    "                                      acc+=math.log(self.trigram[word1][\"__UNK\"][\"__UNK\"])\n",
    "                        else:\n",
    "                            if word2 in self.trigram[\"__UNK\"]:\n",
    "                                if word3 in self.trigram[\"__UNK\"][word2]:\n",
    "                                      acc+=math.log(self.trigram[\"__UNK\"][word2][word3])\n",
    "                                else:\n",
    "                                      acc+=math.log(self.trigram[\"__UNK\"][word2][\"__UNK\"])\n",
    "                            else:\n",
    "                                if word3 in self.trigram[\"__UNK\"][\"__UNK\"]:\n",
    "                                    acc+=math.log(self.trigram[\"__UNK\"][\"__UNK\"][word3])\n",
    "                                else:\n",
    "                                    acc+=math.log(self.trigram[\"__UNK\"][\"__UNK\"][\"__UNK\"])\n",
    "                    return acc, len(tokens[1:])\n",
    "            except KeyError:\n",
    "                return acc, len(tokens[1:]) \n",
    "    def compute_probability(self,filenames=[],methodparams={}):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            if self.verbose:\n",
    "                print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,methodparams=methodparams)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                if self.verbose:\n",
    "                    print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "                else:\n",
    "                    pass\n",
    "        return total_p,total_N\n",
    "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
    "        \"\"\"\n",
    "        compute the probability and length of the corpus\n",
    "        calculate perplexity\n",
    "        lower perplexity means that the model better explains the data\n",
    "        \"\"\"\n",
    "        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
    "        # print(p,N)\n",
    "        if methodparams.get(\"method\") in [\"trigram\", \"quad_gram\"]:\n",
    "            rem = self.super_counter[methodparams.get(\"method\")] - self.magic_counter[methodparams.get(\"method\")]\n",
    "            pp=math.exp(-p/N) * (self.super_counter[methodparams.get(\"method\")]/rem)\n",
    "            return pp\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp \n",
    "    def _make_unknowns(self,known=2):\n",
    "        \"\"\"\n",
    "        Method to distribute probability mass towards the unknown token.\n",
    "        param known (int): dictates cut off point where n-grams less frequent than known are pruned.\n",
    "        \"\"\"\n",
    "        # Unigram -----------------------------------\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v<known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
    "        # Bigram -----------------------------------\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown=self.unigram.get(kk,0)\n",
    "                if isknown <= known:\n",
    "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
    "                    del adict[kk]\n",
    "            isknown=self.unigram.get(k,0)\n",
    "            if isknown <= known:\n",
    "                del self.bigram[k]\n",
    "                current=self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"]=current\n",
    "            else:\n",
    "                self.bigram[k]=adict\n",
    "        # Trigram -----------------------------------\n",
    "        for (k1, dict1) in list(self.trigram.items()):\n",
    "            for (k2, dict2) in list(dict1.items()):\n",
    "                for (k3, val) in list(dict2.items()):\n",
    "                    isknown=self.unigram.get(k3,0)\n",
    "                    if isknown == 0:\n",
    "                        dict2[\"__UNK\"] = dict2.get(\"__UNK\",0) + val\n",
    "                        del dict2[k3]\n",
    "                isknown=self.unigram.get(k2,0)\n",
    "                if isknown <= known:\n",
    "                    del self.trigram[k1][k2]\n",
    "                    current=self.trigram[k1].get(\"__UNK\",{})\n",
    "                    current.update(dict2)\n",
    "                    self.trigram[k1][\"__UNK\"] = current\n",
    "                else:\n",
    "                    self.trigram[k1][k2] = dict2\n",
    "                # For first token:\n",
    "            isknown=self.unigram.get(k1,0)\n",
    "            if isknown <= known:\n",
    "                del self.trigram[k1]\n",
    "                current = self.trigram.get(\"__UNK\",{})\n",
    "                current.update(dict1)\n",
    "                self.trigram[\"__UNK\"] = current \n",
    "            else:\n",
    "                self.trigram[k1] = dict1\n",
    "    def _discount(self,discount=0.75):\n",
    "        #discount each bigram count by a small fixed amount\n",
    "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
    "        \n",
    "        #for each word, store the total amount of the discount so that the total is the same \n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb=len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
    "            \n",
    "        #work out kneser-ney unigram probabilities\n",
    "        #count the number of contexts each word has been seen in\n",
    "        self.kn={}\n",
    "        for (k,adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn[kk]=self.kn.get(kk,0)+1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca672a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=20\n",
    "mylm=language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f305b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037264911964186596"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40df8cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'few remarks thereupon'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate(k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52d9b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018313822342578404"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.compute_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "18ca55f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "unknowns = mylm._make_unknowns()\n",
    "print(unknowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b96b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c4b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=sorted(mylm.unigram.items(),key=lambda x:x[1],reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "241e1579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__START', 0.07091076085352052),\n",
       " ('__END', 0.07091076085352052),\n",
       " (',', 0.05539864076882107),\n",
       " ('the', 0.037264911964186596),\n",
       " ('.', 0.03400149682894974),\n",
       " ('and', 0.024032995841854064),\n",
       " ('to', 0.01867712778033388),\n",
       " ('of', 0.018175595127623227),\n",
       " ('a', 0.01500411325833075),\n",
       " ('I', 0.014230233700404592)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abb734d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26792"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ad2ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Burglars', 8.632231544073153e-07),\n",
       " ('hallway', 8.632231544073153e-07),\n",
       " ('shamming', 8.632231544073153e-07),\n",
       " ('belfry', 8.632231544073153e-07),\n",
       " ('ice-chest', 8.632231544073153e-07),\n",
       " ('cops', 8.632231544073153e-07),\n",
       " ('polling-booth', 8.632231544073153e-07),\n",
       " ('Elevated', 8.632231544073153e-07),\n",
       " ('Reformer', 8.632231544073153e-07),\n",
       " ('down-town', 8.632231544073153e-07)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0899d68",
   "metadata": {},
   "source": [
    "#### WORD simlilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "260dd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - uses gensim version 4.0.1\n",
    "import gensim.downloader as api\n",
    "\n",
    "# fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "# glove_model300 = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3394b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processfiles(max_files=10):\n",
    "    \"\"\"\n",
    "    Code adapted from n_gram_language_model class definition.\n",
    "    Returns lists of preprocessed and not tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentences_preprocessed = []\n",
    "    for afile in tqdm.tqdm(training):\n",
    "        # print(\"Processing {}\".format(afile))\n",
    "        try:\n",
    "            with open(os.path.join(trainingdir,afile)) as instream:\n",
    "                for line in instream:\n",
    "                    line=line.rstrip()\n",
    "                    if len(line)>0:\n",
    "                        tokens = [token for token in tokenize(line) if token not in stop_puncs]\n",
    "                        sentences_preprocessed.append(tokens)\n",
    "                        tokens = [token for token in tokenize(line)]\n",
    "                        sentences.append(tokens)\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"\\nUnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "    return sentences, sentences_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "84693891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21/261 [00:51<04:09,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 48/261 [01:56<04:07,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing LLIFE10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 79/261 [02:58<04:39,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing MFRND10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 131/261 [04:35<01:51,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing ACHOE10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 140/261 [04:43<02:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing 1MLKD11.textClipping: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 178/261 [05:55<01:33,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing TBTAS10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 196/261 [06:23<01:11,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing HFDTR10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 217/261 [07:14<01:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UnicodeDecodeError processing HHOHG10.TXT: ignoring rest of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [08:44<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "sentences, sentence_proccessed  = processfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8cacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_gensim(sentences, window=5, vector_size=100, skip_gram=1, negative=5, alpha=0.05, epochs=15, seed=1):\n",
    "    return Word2Vec(sentences,\n",
    "                  window = window,\n",
    "                  vector_size = vector_size,\n",
    "                  sg = skip_gram,\n",
    "                  negative = negative,\n",
    "                  alpha = alpha,\n",
    "                  epochs = epochs,\n",
    "                  seed = seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "417d390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=60808, vector_size=100, alpha=0.05)\n"
     ]
    }
   ],
   "source": [
    "testing1 = create_word2vec_gensim(sentences)\n",
    "print(testing1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f2652b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=60636, vector_size=100, alpha=0.05)\n"
     ]
    }
   ],
   "source": [
    "testing2 = create_word2vec_gensim(sentence_proccessed)\n",
    "print(testing2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92391725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a834b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5beb4054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [03:47<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "construct_params = {\n",
    "    \"known\" : 5,\n",
    "    \"verbose\" : False,\n",
    "    \"remove_stopwords\" : True\n",
    "}\n",
    "\n",
    "# Initialize n-gram language model.\n",
    "lm=language_model(trainingdir=trainingdir,files=training, test_files=[], construct_params=construct_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "453af523",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_args = {\"n_gram_model\" : lm, \"pre_emb_model\" : word2vec_model300, \"ensemble\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55f5580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I have it from the same source that you are bo...</td>\n",
       "      <td>crying</td>\n",
       "      <td>instantaneously</td>\n",
       "      <td>residing</td>\n",
       "      <td>matched</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>It was furnished partly as a sitting and partl...</td>\n",
       "      <td>daintily</td>\n",
       "      <td>privately</td>\n",
       "      <td>inadvertently</td>\n",
       "      <td>miserably</td>\n",
       "      <td>comfortably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>As I descended , my old ally , the _____ , cam...</td>\n",
       "      <td>gods</td>\n",
       "      <td>moon</td>\n",
       "      <td>panther</td>\n",
       "      <td>guard</td>\n",
       "      <td>country-dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We got off , _____ our fare , and the trap rat...</td>\n",
       "      <td>rubbing</td>\n",
       "      <td>doubling</td>\n",
       "      <td>paid</td>\n",
       "      <td>naming</td>\n",
       "      <td>carrying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>He held in his hand a _____ of blue paper , sc...</td>\n",
       "      <td>supply</td>\n",
       "      <td>parcel</td>\n",
       "      <td>sign</td>\n",
       "      <td>sheet</td>\n",
       "      <td>chorus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           question        a)  \\\n",
       "0  1  I have it from the same source that you are bo...    crying   \n",
       "1  2  It was furnished partly as a sitting and partl...  daintily   \n",
       "2  3  As I descended , my old ally , the _____ , cam...      gods   \n",
       "3  4  We got off , _____ our fare , and the trap rat...   rubbing   \n",
       "4  5  He held in his hand a _____ of blue paper , sc...    supply   \n",
       "\n",
       "                b)             c)         d)             e)  \n",
       "0  instantaneously       residing    matched        walking  \n",
       "1        privately  inadvertently  miserably    comfortably  \n",
       "2             moon        panther      guard  country-dance  \n",
       "3         doubling           paid     naming       carrying  \n",
       "4           parcel           sign      sheet         chorus  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, csv\n",
    "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
    "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
    "\n",
    "with open(questions) as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    lines=list(csvreader)\n",
    "qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e60471f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "tokens=[tokenize(q) for q in qs_df['question']]\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "093322e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class question:\n",
    "\n",
    "    \"\"\"\n",
    "    Question class which stores information about a singular MSR SCC question.\n",
    "\n",
    "    Code adapted from the original work of  Dr. J Weeds, University of Sussex.   \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    aline : str\n",
    "        The training directory where training data can be found.\n",
    "    files : list\n",
    "        List of file names to be trained on.\n",
    "    test_files : list\n",
    "        List of file names for the model to be tested on.\n",
    "    construct_params : dict\n",
    "        Stores the parameters such as known to initialize the language model with.\n",
    "    Attributes\n",
    "    ----------\n",
    "    trainingdir : str\n",
    "        The training directory where training data can be found.\n",
    "    files : list\n",
    "        List of file names to be trained on.\n",
    "    test_files : list\n",
    "        List of file names for the model to be tested on.\n",
    "    construct_params : dict\n",
    "        Stores the parameters such as known to initialize the language model with.\n",
    "    verbose : bool\n",
    "        Whether or not method calls will print progress.\n",
    "    unigram : dict\n",
    "        Dictionary to store unigram probabilities.\n",
    "    bigram : dict\n",
    "        Dictionary to store bigram probabilities.\n",
    "    trigram : dict\n",
    "        Dictionary to store trigram probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self,aline,stop=True):\n",
    "        self.fields=aline\n",
    "        self.num2letter = {\n",
    "            0:\"a\",\n",
    "            1:\"b\",\n",
    "            2:\"c\",\n",
    "            3:\"d\",\n",
    "            4:\"e\"\n",
    "            }\n",
    "        self.stop = stop\n",
    "        if self.stop:\n",
    "            self.tokenized = [token.lower() for token in tokenize(self.fields[1]) if token.lower() not in stop_puncs]\n",
    "          # self.tokenized = [wordnet_lemmatizer.lemmatize(token) for token in self.tokenized]\n",
    "        else:\n",
    "            self.tokenized = tokenize(self.fields[1])\n",
    "        self.options = self.fields[2:7]\n",
    "        self.backoff_factor = 0.4\n",
    "          \n",
    "\n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "\n",
    "    def add_answer(self,fields):\n",
    "        self.answer=fields[1]\n",
    "   \n",
    "\n",
    "    def get_context(self,window,target=\"_____\",method=\"left\"):\n",
    "        \"\"\"\n",
    "        Method to return the context of a target word in question sentence. \n",
    "        If not sufficient context the method returns context with unknown token padding.\n",
    "        \"\"\"\n",
    "        for i, token in enumerate(self.tokenized):\n",
    "            if token == target:\n",
    "                if method==\"left\":\n",
    "                    try:\n",
    "                        return self.tokenized[i-window:i]\n",
    "                    except:\n",
    "                        return [\"__UNK\"] * window\n",
    "                elif method==\"right\": \n",
    "                    return self.tokenized[i+1:i+1+window]\n",
    "\n",
    "\n",
    "    def chooseA(self):\n",
    "        return(\"a\")\n",
    "\n",
    "\n",
    "    def random(self):\n",
    "        \"\"\"\n",
    "        Retrun random choice of letter.\n",
    "        \"\"\"\n",
    "        return random.choice(self.num2letter)\n",
    "\n",
    "\n",
    "    def unigram(self):\n",
    "        \"\"\"\n",
    "        Return position of word with greatest unigram probability. 0 otherwise.\n",
    "        \"\"\"\n",
    "        option_probs = [lm.unigram[word] if word in lm.unigram else 0 for word in self.options]\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def bigram(self, context_dir=\"left\"): # Backoff\n",
    "        \"\"\"\n",
    "        Return position of word-pair with greatest bigram probability. 0 otherwise. \n",
    "        \"\"\"\n",
    "        option_probs = []\n",
    "        context = self.get_context(1, method=context_dir) # [0] to delist context.\n",
    "        context = [\"__UNK\"] + context\n",
    "        if context_dir == \"left\":\n",
    "            for word in self.options:\n",
    "              # Bigram.\n",
    "                if context[-1] in lm.bigram and word in lm.bigram[context[-1]]:\n",
    "                    option_probs.append(lm.bigram[context[-1]][word])\n",
    "              # Back off to unigram\n",
    "                elif word in lm.unigram:\n",
    "                    option_probs.append(self.backoff_factor * lm.unigram[word])\n",
    "                else:\n",
    "                    option_probs.append(0)\n",
    "        elif context_dir == \"right\":\n",
    "            option_probs = [lm.bigram[word][context] if word in lm.bigram and context in lm.bigram[word] else 0 for word in self.options]\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def trigram(self, context_dir=\"left\"): # Backoff\n",
    "        \"\"\"\n",
    "        Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "        \"\"\"\n",
    "        option_probs = []\n",
    "        context = self.get_context(2, method=context_dir)\n",
    "        context = [\"__UNK\"] * 2 + context \n",
    "        if context_dir == \"left\":\n",
    "            for word in self.options:\n",
    "                if context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and word in lm.trigram[context[-2]][context[-1]]:\n",
    "                    option_probs.append(lm.trigram[context[-2]][context[-1]][word])\n",
    "                # Back off to bigram.\n",
    "                elif context[-1] in lm.bigram and word in lm.bigram[context[-1]]:\n",
    "                    option_probs.append(self.backoff_factor * lm.bigram[context[-1]][word])\n",
    "                # Back off to unigram.\n",
    "                elif word in lm.unigram:\n",
    "                    option_probs.append(self.backoff_factor * self.backoff_factor * lm.unigram[word])\n",
    "                # Else 0.\n",
    "                else:\n",
    "                    option_probs.append(0)\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "   \n",
    "\n",
    "    def simple_4_gram(self, additional_args={}):\n",
    "        lm = additional_args.get(\"n_gram_model\")\n",
    "        option_probs = []\n",
    "        left_context = self.get_context(3, method=\"left\")\n",
    "        left_context = [\"__UNK\"] * 3 + left_context\n",
    "        con1 = left_context[-3]\n",
    "        con2 = left_context[-2]\n",
    "        con3 = left_context[-1]\n",
    "        right_context = self.get_context(3, method=\"right\")\n",
    "        right_context = right_context + [\"__UNK\"] * 3\n",
    "        r_con1 = right_context[0]\n",
    "        r_con2 = right_context[1]\n",
    "        r_con3 = right_context[2]\n",
    "        for word in self.options:\n",
    "            try:\n",
    "                score = 0\n",
    "              # Bigram\n",
    "                if word in mylm.bigram.get(con3,{}):\n",
    "                    score += 1\n",
    "                if r_con1 in mylm.bigram.get(word,{}):\n",
    "                    score += 1\n",
    "              # Trigram\n",
    "                if word in mylm.trigram.get(con3,{}).get(con2,{}):\n",
    "                    score += 2\n",
    "                if r_con1 in mylm.trigram.get(con3,{}).get(word,{}):\n",
    "                    score += 2\n",
    "                if r_con2 in mylm.trigram.get(word,{}).get(r_con1,{}):\n",
    "                    score += 2\n",
    "                option_probs.append(score)\n",
    "            except TypeError:\n",
    "                print([con1,con2,con3,word,r_con1,r_con2,r_con3])\n",
    "                option_probs.append(0)\n",
    "          # -------------\n",
    "          # Ensemble\n",
    "        if additional_args.get(\"ensemble\", False):\n",
    "            return option_probs\n",
    "        # -------------\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def embedding_similarity(self, method=\"cos\", additional_args={}):\n",
    "        \"\"\"\n",
    "        For use with pretrained or even custom embeddings.\n",
    "        \"\"\"\n",
    "        model = additional_args.get(\"pre_emb_model\")\n",
    "        option_probs = []\n",
    "        # Remove target string.\n",
    "        sentence = self.tokenized #.remove(\"_____\")\n",
    "        # Iterate through candidate choices.\n",
    "        for word in self.options:\n",
    "            try:\n",
    "                # If no embedding for that word exists.\n",
    "                if word not in word_vectors:\n",
    "                    option_probs.append(0)\n",
    "                # Continue to next candidate word.\n",
    "                    continue\n",
    "                # Get vectorized form of word.\n",
    "                word_vector = word_vectors.get_vector(word)\n",
    "                # Get vectorized form of sentence tokens.\n",
    "                sentence_vectors = [word_vectors.get_vector(sent_token) for sent_token in sentence if sent_token in model.wv and sent_token != \"_____\"]\n",
    "                # For euclidean distances.\n",
    "                if method == \"euc\":\n",
    "                    sim_score = [np.linalg.norm(word_vectors.get_vector(word) - word_vectors.get_vector(sent_token)) for sent_token in sentence if sent_token in word_vectors and sent_token != \"_____\"]\n",
    "                # For cosine distances.\n",
    "                else:\n",
    "                    sim_score = word_vectors.cosine_similarities(word_vector, sentence_vectors)\n",
    "                # Append average \"method\" similarity.\n",
    "                option_probs.append(sum(sim_score)/len(sim_score))\n",
    "            except (TypeError, np.AxisError, ZeroDivisionError) as e:\n",
    "                print(sentence)\n",
    "                option_probs.append(0)\n",
    "              # ----------------\n",
    "              # Ensemble - cosine:\n",
    "        if additional_args.get(\"ensemble\", False):\n",
    "            return option_probs\n",
    "      # ----------------\n",
    "        if method == \"cos\":\n",
    "            index = option_probs.index(max(option_probs))\n",
    "        else:\n",
    "            index = option_probs.index(min(option_probs))\n",
    "        return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def ensemble(self, additional_args={}):\n",
    "        \"\"\"\n",
    "        Ensemble method which aggregates scores of both tested models and normalizes + sums them.\n",
    "        \"\"\"\n",
    "        n_gram_model = additional_args.get(\"n_gram_model\")\n",
    "        n_gram = self.simple_4_gram(additional_args=additional_args)\n",
    "        norm_n_gram = [float(i)/sum(n_gram) if sum(n_gram) !=0 else 0 for i in n_gram]\n",
    "\n",
    "        pre_emb_model = additional_args.get(\"pre_emb_model\")\n",
    "        pre_emb = self.embedding_similarity(additional_args=additional_args)\n",
    "        norm_pre_emb = [float(i)/sum(pre_emb) if sum(pre_emb) !=0 else 0 for i in pre_emb]\n",
    "\n",
    "        option_probs = [sum(val) for val in zip(norm_n_gram, norm_pre_emb)]\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "\n",
    "\n",
    "  # ################################################### comment below to use stupid backoff.\n",
    "    def bigram(self, context_dir=\"left\"):\n",
    "        \"\"\"\n",
    "        Return position of word-pair with greatest bigram probability. 0 otherwise. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            context = self.get_context(1, method=context_dir)[0] # [0] to delist context.\n",
    "        except:\n",
    "            context = [\"__START\"][0] \n",
    "        if context_dir == \"left\":\n",
    "            option_probs = [lm.bigram[context][word] if context in lm.bigram and word in lm.bigram[context] else 0 for word in self.options]\n",
    "        elif context_dir == \"right\":\n",
    "            option_probs = [lm.bigram[word][context] if word in lm.bigram and context in lm.bigram[word] else 0 for word in self.options]\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def trigram(self, context_dir=\"left\"): \n",
    "        \"\"\"\n",
    "        Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "        \"\"\"\n",
    "        option_probs = []\n",
    "        try:\n",
    "            context = [\"__UNK\"] * 2 + self.get_context(2, method=context_dir)\n",
    "        except:\n",
    "            context = [\"__UNK\"] * 2 + context\n",
    "        if context_dir == \"left\":\n",
    "            for word in self.options:\n",
    "                if context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and word in lm.trigram[context[-2]][context[-1]]:\n",
    "                    option_probs.append(lm.trigram[context[-2]][context[-1]][word])\n",
    "                elif context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and \"__UNK\" in lm.trigram[context[-2]][context[-1]]:\n",
    "                    option_probs.append(lm.trigram[context[-2]][context[-1]][\"__UNK\"])\n",
    "                # Else 0.\n",
    "                else:\n",
    "                    option_probs.append(0)\n",
    "        index = option_probs.index(max(option_probs))\n",
    "        return self.num2letter[index]\n",
    "    \n",
    "  # ################################################### \n",
    "    \n",
    "    \n",
    "    def predict(self, method=\"chooseA\", additional_args=None):\n",
    "        if method==\"chooseA\":\n",
    "            return self.chooseA()\n",
    "        elif method==\"random\":\n",
    "            return self.random()\n",
    "        elif method==\"unigram\":\n",
    "            return self.unigram()\n",
    "        elif method==\"bigram\":\n",
    "            return self.bigram()\n",
    "        elif method==\"trigram\":\n",
    "            return self.trigram()\n",
    "        elif method==\"quad_gram\":\n",
    "            return self.quad_gram()\n",
    "        elif method==\"simple_4_gram\":\n",
    "            return self.simple_4_gram(additional_args=additional_args)\n",
    "        elif method==\"embedding_similarity\":\n",
    "            return self.embedding_similarity(additional_args=additional_args)\n",
    "        elif method==\"ensemble\":\n",
    "            return self.ensemble(additional_args=additional_args)\n",
    "        elif method == \"cos\":\n",
    "            return self.embedding_similarity(additional_args=additional_args)\n",
    "        elif method == \"euc\":\n",
    "            return self.embedding_similarity(additional_args=args,method=\"euc\")\n",
    "\n",
    "\n",
    "    def predict_and_score(self, method=\"chooseA\", additional_args=None):\n",
    "        #compare prediction according to method with the correct answer\n",
    "        #return 1 or 0 accordingly\n",
    "        # Method also records which questions were answered correctly by index.\n",
    "        prediction=self.predict(method=method, additional_args=additional_args)\n",
    "        if prediction == self.answer:\n",
    "            correct_answers.get(method).append(1)\n",
    "            return 1\n",
    "        else:\n",
    "            correct_answers.get(method).append(0)\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40e6ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scc_reader:\n",
    "    \n",
    "\n",
    "    def __init__(self,qs=questions,ans=answers,stop=False):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.stop = stop\n",
    "        self.read_files()\n",
    "\n",
    "\n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline, self.stop) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "\n",
    "\n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "    \n",
    "\n",
    "    def predict(self,method=\"chooseA\"):\n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "    \n",
    "    def predict_and_score(self,method=\"chooseA\", additional_args=None):\n",
    "        scores=[q.predict_and_score(method=method, additional_args=additional_args) for q in self.questions]\n",
    "        return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d08b6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = {\"simple_4_gram\" : [], \"embedding_similarity\" : [], \"ensemble\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1cb07f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23461538461538461"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple-4-gram score\n",
    "additional_args = {\"n_gram_model\" : mylm, \"pre_emb_model\" : word2vec_model300, \"ensemble\" : False}\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"simple_4_gram\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54e284ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23461538461538461"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple-4-gram score\n",
    "additional_args = {\"trigram\" : lm, \"pre_emb_model\" : word2vec_model300, \"ensemble\" : False}\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"simple_4_gram\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "593af0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23461538461538461"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple-4-gram score\n",
    "additional_args = {\"bigram\" : lm, \"pre_emb_model\" : word2vec_model300, \"ensemble\" : False}\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"simple_4_gram\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44d075d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19903846153846153"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding similarity score\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"embedding_similarity\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bf7dd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23461538461538461"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble score\n",
    "additional_args[\"ensemble\"] = True\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"ensemble\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1f025",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4d7be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(action='ignore') #,category=DeprecationWarning,module='gensim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45974423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the answer file\n",
    "holding_list = []\n",
    "with open(\"/Users/pushpanb/Downloads/Second Sem/ANLP/lab2resources/sentence-completion/test_answer.csv\") as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    alines=list(csvreader)\n",
    "    holding_list.append(alines)\n",
    "answers = [holding_list[0][i][1] for i, _ in enumerate(holding_list[0]) if i != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13d1cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All correct.\n",
    "correct = []\n",
    "# All incorrect.\n",
    "incorrect = []\n",
    "# N-gram incorrect\n",
    "n_g_incorrect = []\n",
    "# n-g over emb.\n",
    "n_g_vs_emb = []\n",
    "# emb over n-g.\n",
    "emb_vs_n_g = []\n",
    "# ensemble got right when other two didnt.\n",
    "ensemble_solved = []\n",
    "# Ensemble favouring n-gram.\n",
    "n_g_favour = []\n",
    "# Ensemble favouring word embeddings.\n",
    "emb_favour = []\n",
    "# Ensemble only.\n",
    "ensemble_only = []\n",
    "\n",
    "for i, (n_g, emb, ensemble) in enumerate(zip(correct_answers[\"simple_4_gram\"], correct_answers[\"embedding_similarity\"], correct_answers[\"ensemble\"])):\n",
    "    if n_g == emb == ensemble == 1:\n",
    "        correct.append(i)\n",
    "    if n_g == emb == ensemble == 0:\n",
    "        incorrect.append(i)\n",
    "    if n_g == 1 and emb == 0:\n",
    "        n_g_vs_emb.append(i)\n",
    "    if n_g == 0 and emb == 1:\n",
    "        emb_vs_n_g.append(i)\n",
    "    if n_g == emb == 0 and ensemble == 1:\n",
    "        ensemble_solved.append(i)\n",
    "    if n_g == 0:\n",
    "        n_g_incorrect.append(i)\n",
    "    if n_g == 0 and ensemble == 1 == emb:\n",
    "        emb_favour.append(i)\n",
    "    if emb == 0 and ensemble == 1 == n_g:\n",
    "        n_g_favour.append(i)\n",
    "\n",
    "    if ensemble == 1:\n",
    "        ensemble_only.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dc8b57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(index):\n",
    "    \"\"\"\n",
    "    Function to print out the answers predicted by each model stated, as well as the question (tokenized and not) and the correct answer.\n",
    "    \"\"\"\n",
    "\n",
    "    q = question([SCC.get_field(\"id\")[index], SCC.get_field(\"question\")[index], \n",
    "                  SCC.get_field(\"a)\")[index], SCC.get_field(\"b)\")[index],\n",
    "                  SCC.get_field(\"c)\")[index], SCC.get_field(\"d)\")[index], SCC.get_field(\"e)\")[index]])\n",
    "    answer = answers[index]\n",
    "    additional_args = {\"n_gram_model\" : lm, \"pre_emb_model\" : word2vec_model300, \"ensemble\" : False}\n",
    "    pred_ngram = q.predict(\"simple_4_gram\", additional_args=additional_args)\n",
    "    answer_ngram = SCC.get_field(\"{})\".format(pred_ngram))[index]\n",
    "  \n",
    "    pred_emb = q.predict(\"embedding_similarity\", additional_args=additional_args)\n",
    "    answer_emb = SCC.get_field(\"{})\".format(pred_emb))[index]\n",
    "  \n",
    "    additional_args[\"ensemble\"] = True\n",
    "    pred_ensemble = q.predict(\"ensemble\", additional_args=additional_args)\n",
    "    answer_ensemble = SCC.get_field(\"{})\".format(pred_ensemble))[index]\n",
    "\n",
    "    answer_correct = SCC.get_field(\"{})\".format(answers[index]))[index]\n",
    "\n",
    "    # print()\n",
    "    print(\"------------------------------\")\n",
    "    print(SCC.get_field(\"question\")[index])\n",
    "    print(q.tokenized)\n",
    "    print()\n",
    "    print(answer_correct)\n",
    "    print()\n",
    "    print(\"N-gram: {}\".format(answer_ngram))\n",
    "    print(\"Embedd: {}\".format(answer_emb))\n",
    "    print(\"Ensemb: {}\".format(answer_ensemble))\n",
    "    print(\"------------------------------\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e265796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.\n",
      "['got', '_____', 'fare', 'trap', 'rattled', 'back', 'way', 'leatherhead']\n",
      "\n",
      "carrying\n",
      "\n",
      "N-gram: carrying\n",
      "Embedd: rubbing\n",
      "Ensemb: carrying\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check = error_analysis(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e58126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3457f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:11<00:00,  1.12it/s]\n",
      "100%|██████████| 80/80 [01:12<00:00,  1.10it/s]\n",
      "100%|██████████| 80/80 [01:17<00:00,  1.03it/s]\n",
      "100%|██████████| 80/80 [01:18<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example development code for experimenting with the known parameter.\n",
    "lm_known = {}\n",
    "MAX_FILES=100\n",
    "\n",
    "training_shuffled = shuffle(training)\n",
    "training_shuffled = training_shuffled[:MAX_FILES]\n",
    "\n",
    "num = 0.2\n",
    "train, test = train_test_split(training_shuffled,test_size=num)\n",
    "\n",
    "for known in [5, 10, 25, 50]:\n",
    "\n",
    "\n",
    "    construct_params = {\n",
    "      \"known\" : known,\n",
    "      \"verbose\" : False,\n",
    "      \"remove_stopwords\" : True\n",
    "    }\n",
    "\n",
    "    # Initialize n-gram language model.\n",
    "    lm=language_model(trainingdir=trainingdir,files=train, test_files=test, construct_params=construct_params)\n",
    "\n",
    "    lm_known[known] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec911cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowns_test = [lm.compute_perplexity(filenames=lm.test_files,methodparams={\"method\":method}) for lm in lm_stop.values() for method in [\"unigram\", \"bigram\", \"trigram\", \"quad_gram\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2bf8c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(lst, n):\n",
    "    \"\"\"\n",
    "    Chunnk lst (list) into n chunks.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e246ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = [len(lm.unigram) for lm in lm_known.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8497913",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = []\n",
    "two = []\n",
    "three = []\n",
    "four = []\n",
    "for i_l in list(chunker(lm_known,3)):\n",
    "    one.append(i_l[0])\n",
    "    two.append(i_l[1])\n",
    "    three.append(i_l[2])\n",
    "    four.append(i_l[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc = pd.DataFrame({\n",
    "    'Training Doc Size': [key for key, lm in lm_known.items()], \n",
    "    'unigram': one,\n",
    "    'bigram': two,\n",
    "    \"trigram\":three,\n",
    "    'vocab': vocab_size,\n",
    "    })\n",
    "\n",
    "data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing function.\n",
    "ax = sns.lineplot(x='knowns', y='value', hue='variable', \n",
    "             data=pd.melt(data_preproc, ['knowns']))\n",
    "\n",
    "ax.set(xlabel=\"Known Parameter Value\", ylabel=\"Perplexity\")\n",
    "# ax._legend.set_title(\"N-Gram\")\n",
    "leg = ax.legend()\n",
    "leg.set_title(\"N-Gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51540b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "keys = [p[0] for p in gg]\n",
    "val = [p[1] for p in gg]\n",
    "\n",
    "ax = sns.barplot(y=keys, x=val, orient=\"horizontal\")\n",
    "ax.set(xlabel=\"Frequency\", ylabel=\"Word Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdbf6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
